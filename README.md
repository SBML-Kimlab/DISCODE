# DISCODE
![discode_img_2@4x](https://github.com/SBML-Kimlab/DISCODE/assets/153895812/b9a46ca6-7727-40a3-b345-61d19af44a37)



**DISCODE** (**D**eep learning-based **I**terative pipline to analyze **S**pecificity of **CO**factors and to **D**esign **E**nzyme) is a transformer-based NAD/NADP classification model. The model uses ESM-2 language model for amino acid embedding, finally represents the probability of NAD and NADP. This code can also be run on Google Colaboratory.

## Installation
**Note : This code was developed in Linux, and has been tested in Ubuntu 18.04 and 20.04 with Python 3.8.**
1. Clone github repository
```
git clone https://github.com/SBML-Kimlab/DISCODE.git
```
2. Create and activate virtual environment
```
cd DISCODE
conda env create -f discode.yaml
conda activate discode
```

## Usage
**Example of classification**
```python
from discode import models, utils

model_path = "weights/weights.pt" # please specify the model weight path
model = models.load(model_path) # if gpu available, it will automatically load on gpu

name, sequence = "Q9K3J3", "MTRTPVNVTVTGAAGQIGYALLFRIASGQLLGADVPVKLRLLEITPALKAAEGTAMELDDCAFPLLQGIEITDDPNVAFDGANVALLVGARPRTKGMERGDLLEANGGIFKPQGKAINDHAADDIKVLVVGNPANTNALIAQAAAPDVPAERFTAMTRLDHNRALTQLAKKTGSTVADIKRLTIWGNHSATQYPDIFHATVAGKNAAETVNDEKWLADEFIPTVAKRGAAIIEARGASSAASAANAAIDHVYTWVNGTAEGDWTSMGIPSDGSYGVPEGIISSFPVTTKDGSYEIVQGLDINEFSRARIDASVKELSEEREAVRGLGLI"

# Predict label of wildtype sequence
# The sequence will be preprocessed with ESM-2 model
data = utils.tokenize_and_dataloader(name, sequence)

# The processed data will be transferred into the the model, and predict the probability, attention weights, outlier residues
outlier_idx, probability, predicted_label, _name, attention_weights = utils.model_processing(data, model)
# The outlier_idx is zero-index
# The _name is the same as the previously declared variable name.


# This will plot maximum attention map of overall model, in shape of [8,20]
utils.make_max_attention_map(attention_weights)

# Plot the attention sum and outlier residues
# This will plot attention sum, in shape of sequence length L
utils.plot_attention_sum(attention_weights, outlier_idx, sequence)
```

**Ipynb example of a designing pipeline for cofactor switching mutants is in example/example.ipynb.**
[Options]
  - max_num_mutation: set the maximum number of mutations to yield cofactor switching mutant (default=3)
  - max_num_solution: set the maximum number of solutions to return (default=50)
  - prob_thres: set a probability threshold for cofactor specificity reversal (default=0.5)
  - pickle_path: directory where a pickle file is saved (default='.')
  - sequence: a protein sequence aimed at changing cofactor specificity
  - name: sequence id (default='unknown')
  - mode (default=iterative_num):
    * iterative_prob : scan all combinations of mutations guided by attention analysis and return those for optimal probabilities (exhaustively calculate most probable designs).
    * iterative_num : scan all combinations of residues guided by attention analysis. However, if a cofactor switching design is obtained, scan only to combinations from the same number of mutations (exhaustively calculate a minimal requirement of designs).
    * shortest : scan minimum number of combinations by selecting mutations that show optimal probability changes (fastest search).

**The results generated by each mutation step will be saved on pickle_path as {name_mode_mutation_step}.pkl**
```python
utils.scan_switch_mutation(model = model,
                           max_num_mutation = 3,
                           name = name,
                           sequence = sequence,
                           mode = "iterative_num",)
```

## Contact
If you have any questions, problems or suggestions, please contact [us](https://sites.google.com/view/systemskimlab/home).

## Reference
